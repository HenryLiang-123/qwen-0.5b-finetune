{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", temperature = 0.1, top_p = 0.6, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the YAML data\n",
    "with open(\"./me.yaml\", \"r\") as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "# Process the education data\n",
    "education = data[\"education\"]\n",
    "education_text = \"\\n\".join(\n",
    "    [f\"- Degree: {e['degree']}, Institution: {e['institution']}, Graduation Date: {e['graduation_date']}\" for e in education]\n",
    ")\n",
    "\n",
    "# Process the work experience data\n",
    "work = data[\"work_experience\"]\n",
    "work_text = \"\\n\".join(\n",
    "    [\n",
    "        f\"- Role: {w['title']}, Company: {w['company']}, Start Date: {w['start_date']}, End Date: {w.get('end_date', 'Present')}, Work Description: {w['achievements']}\"\n",
    "        for w in work\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Process the skills data\n",
    "skills = data[\"skills\"]\n",
    "skills_text = (\n",
    "    f\"Programming: {skills['programming_languages']}, Tools: {skills['tools']}, Languages: {skills['languages']}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You are an AI assistant of Henry Liang.\n",
    "You are designed to assist users with questions about Henry Liang, his work, and his experience.\n",
    "\n",
    "**Tone and Scope:**\n",
    "Answer all questions cheerfully, but do not provide more information than what is explicitly asked.\n",
    "\n",
    "**Uncertainty:**\n",
    "If you do not know the answer, state clearly, \"I do not know.\" Avoid guessing or fabricating information.\n",
    "\n",
    "**Restrictions:**\n",
    "Do not answer questions that are inappropriate, harmful, racist, or illegal.\n",
    "Avoid using inappropriate language under any circumstance.\n",
    "Do not provide medical, legal, or financial advice.\n",
    "Do not share any information that can identify or locate a person.\n",
    "Follow these guidelines strictly, and always prioritize clarity, accuracy, and adherence to the scope of your role.\n",
    "'''\n",
    "\n",
    "def get_user_prompt(education_text, work_text, skills_text, q):\n",
    "    # Combine the information into the final content string\n",
    "    content = f\"\"\"\n",
    "    The following is data about Henry Liang:\n",
    "\n",
    "    **Education:**\n",
    "    {education_text}\n",
    "\n",
    "    **Work Experience:**\n",
    "    {work_text}\n",
    "\n",
    "    **Skills:**\n",
    "    {skills_text}\n",
    "\n",
    "    If it is a question about Henry Liang, use the above information to answer the following question about Henry Liang.\n",
    "\n",
    "    {q}\n",
    "    \"\"\"\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = [\n",
    "    # Education-related questions\n",
    "    {\n",
    "        \"question\": \"Where did Henry complete his master's degree?\",\n",
    "        \"answer\": \"Henry completed his M.S. in Machine Learning and Data Science at Northwestern University, graduating in December 2023.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was his undergraduate degree in?\",\n",
    "        \"answer\": \"His undergraduate degree was a B.S. in Applied Mathematics and Statistics from the University of California, Los Angeles, which he completed in March 2022.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which institutions did Henry attend for his higher education?\",\n",
    "        \"answer\": \"Henry attended Northwestern University for his master's degree and the University of California, Los Angeles, for his bachelor's degree.\"\n",
    "    },\n",
    "\n",
    "    # Work experience-related questions\n",
    "    {\n",
    "        \"question\": \"What is Henry's current job title?\",\n",
    "        \"answer\": \"Henry's current job title is Data Scientist II at Vail Systems, Inc.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What major achievement did Henry accomplish at Vail Systems?\",\n",
    "        \"answer\": \"Henry created a novel hybrid embedding that reduced the no-context rate for RFP completion by 83.33%, leading to a paper acceptance and invited talk at the AI-ML Systems Conference in October 2024.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What framework did Henry use to build the FAQ chatbot at Vail Systems?\",\n",
    "        \"answer\": \"He used a Retrieval-Augmented Generation (RAG) framework to build the FAQ chatbot.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was the improvement in FAQ system latency that Henry achieved at Vail Systems?\",\n",
    "        \"answer\": \"Henry reduced FAQ system latency by 65% through data curation, embedding optimization, and HNSW indexing with PostgreSQL.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What did Henry work on during his internship at Amazon?\",\n",
    "        \"answer\": \"During his Amazon internship, Henry developed a custom LLM app to explain knowledge graph differences and detect network routing anomalies, achieving 99%\\ accuracy in summarizing path-related attributes with KGAG.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which company did Henry intern at before Amazon?\",\n",
    "        \"answer\": \"Before Amazon, Henry interned at Roblox as a Data Science Intern.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What early warning system did Henry develop at Roblox?\",\n",
    "        \"answer\": \"Henry developed a 94%\\ accurate early warning system using an XGBoost model to forecast significant impression shifts.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What real-time forecasting system did Henry design at Northwestern University?\",\n",
    "        \"answer\": \"Henry designed a scalable AWS IoT streaming pipeline for real-time time-series forecasting, achieving a throughput of 55 predictions per second.\"\n",
    "    },\n",
    "\n",
    "    # Skills and tools-related questions\n",
    "    {\n",
    "        \"question\": \"Which programming languages is Henry proficient in?\",\n",
    "        \"answer\": \"Henry is proficient in Python, R, PostgreSQL, Java, and JavaScript.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What big data tools does Henry have experience with?\",\n",
    "        \"answer\": \"Henry has experience with Apache Spark and Apache Hadoop.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What containerization tools does Henry use?\",\n",
    "        \"answer\": \"Henry uses Docker and Kubernetes for containerization.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What languages can Henry speak?\",\n",
    "        \"answer\": \"Henry speaks English, Mandarin, and French.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What tools has Henry used for machine learning?\",\n",
    "        \"answer\": \"Henry has used TensorFlow, PyTorch, LangChain, and Hugging Face for machine learning.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What cloud service did Henry use in his real-time forecasting project?\",\n",
    "        \"answer\": \"Henry used AWS IoT pipelines and AWS EC2 for his real-time forecasting project.\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d420b48edd14de2af1c9c47094dbe54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46eb7461a90c4b6c9acd17ccf09308b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/852 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7b4638ef144d92aaf4b335bd63e102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db4d43e36124e81ad893ee04386d270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1838e100f3f44f1aace80e9066e84e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-English were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.99998474, 'index': 4, 'word': '▁Wolfgang', 'start': 11, 'end': 19}, {'entity': 'I-LOC', 'score': 0.9999943, 'index': 9, 'word': '▁Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-English\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-English\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Entities: [('December', 'DATE'), ('2023', 'DATE'), ('XYZ University', 'ORG')]\n",
      "Hypothesis Entities: [('2023', 'DATE'), ('ABC University', 'ORG'), ('PhD', 'WORK_OF_ART'), ('XYZ University', 'ORG')]\n",
      "Hallucinated Entities: [('ABC University', 'ORG'), ('PhD', 'WORK_OF_ART')]\n",
      "Non-Hallucinated Entities: [('2023', 'DATE'), ('XYZ University', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "# # prompt = '''He has used the following tools for big data processing:\n",
    "# # - TensorFlow\n",
    "# # - PyTorch\n",
    "# # - LangChain\n",
    "# # - Hugging Face\n",
    "# # - Git\n",
    "# # - Docker\n",
    "# # - Kubernetes\n",
    "# # - Apache Spark\n",
    "# # - Apache Hadoop'''\n",
    "# # hypothesis = \"You graduated in 2023 with a master's from ABC University, and currently doing a PhD at XYZ University.\"\n",
    "\n",
    "# # # Prompt and hypothesis\n",
    "# prompt = \"I graduated with a master's in December 2023 from XYZ University.\"\n",
    "# hypothesis = \"You graduated in 2023 with a master's from ABC University, and currently doing a PhD at XYZ University.\"\n",
    "\n",
    "# # Extract entities\n",
    "# def extract_entities(text):\n",
    "#     doc = nlp(text)\n",
    "#     return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# def extract_dates(entities):\n",
    "#     new_entities = []\n",
    "#     for ent, label in entities:\n",
    "#         if label == \"DATE\":\n",
    "#             splits = ent.split(\" \")\n",
    "#             for split in splits:\n",
    "#                 new_entities.append((split, label))\n",
    "#         else:\n",
    "#             new_entities.append((ent, label))\n",
    "#     return new_entities\n",
    "\n",
    "# prompt_entities = extract_dates(extract_entities(prompt))\n",
    "# hypothesis_entities = extract_dates(extract_entities(hypothesis))\n",
    "\n",
    "# # Compare entities\n",
    "# hallucinated = [ent for ent in hypothesis_entities if ent not in prompt_entities]\n",
    "# non_hallucinated = [ent for ent in hypothesis_entities if ent in prompt_entities]\n",
    "\n",
    "\n",
    "# print(\"Prompt Entities:\", prompt_entities)\n",
    "# print(\"Hypothesis Entities:\", hypothesis_entities)\n",
    "# print(\"Hallucinated Entities:\", hallucinated)\n",
    "# print(\"Non-Hallucinated Entities:\", non_hallucinated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Entities: ['December', '2023', 'XYZ University']\n",
      "Hypothesis Entities: ['PhD', 'ABC University', '2023', 'XYZ University']\n",
      "Hallucinated Entities: ['PhD', 'ABC University']\n",
      "Non-Hallucinated Entities: ['2023', 'XYZ University']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# # Prompt and hypothesis\n",
    "prompt = \"I graduated with a master's in December 2023 from XYZ University.\"\n",
    "hypothesis = \"You graduated in 2023 with a master's from ABC University, and currently doing a PhD at XYZ University.\"\n",
    "\n",
    "nlp_roberta = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy='simple')\n",
    "\n",
    "def extract_entities_roberta(text):\n",
    "    doc = nlp_roberta(text)\n",
    "    return [(ent['word'], ent['entity_group']) for ent in doc]\n",
    "\n",
    "# Extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "def extract_dates(entities):\n",
    "    new_entities = []\n",
    "    for ent, label in entities:\n",
    "        if label == \"DATE\":\n",
    "            splits = ent.split(\" \")\n",
    "            for split in splits:\n",
    "                new_entities.append((split, label))\n",
    "        else:\n",
    "            new_entities.append((ent, label))\n",
    "    return new_entities\n",
    "\n",
    "def get_all_entities(prompt, hypothesis):\n",
    "    prompt_entities = extract_dates(extract_entities(prompt))\n",
    "    hypothesis_entities = extract_dates(extract_entities(hypothesis))\n",
    "\n",
    "    prompt_entities_roberta = extract_dates(extract_entities_roberta(prompt))\n",
    "    hypothesis_entities_roberta = extract_dates(extract_entities_roberta(hypothesis))\n",
    "\n",
    "    prompt_entities = [t[0] for t in prompt_entities]\n",
    "    prompt_entities_roberta = [t[0] for t in prompt_entities_roberta]\n",
    "\n",
    "    hypothesis_entities = [t[0] for t in hypothesis_entities]\n",
    "    hypothesis_entities_roberta = [t[0] for t in hypothesis_entities_roberta]\n",
    "\n",
    "    combined_prompt_entities = list(set(prompt_entities_roberta + prompt_entities))\n",
    "    combined_hypothesis_entities = list(set(hypothesis_entities_roberta + hypothesis_entities))\n",
    "    # Compare entities\n",
    "    hallucinated = [ent for ent in combined_hypothesis_entities if ent not in combined_prompt_entities]\n",
    "    non_hallucinated = [ent for ent in combined_hypothesis_entities if ent in combined_prompt_entities]\n",
    "\n",
    "    return combined_prompt_entities, combined_hypothesis_entities, hallucinated, non_hallucinated\n",
    "\n",
    "all_entities = get_all_entities(prompt, hypothesis)\n",
    "\n",
    "print(\"Prompt Entities:\", all_entities[0])\n",
    "print(\"Hypothesis Entities:\", all_entities[1])\n",
    "print(\"Hallucinated Entities:\", all_entities[2])\n",
    "print(\"Non-Hallucinated Entities:\", all_entities[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where did Henry complete his master's degree?\n",
      "Generated Answer: Henry completed his Master's degree from Northwestern University.\n",
      "Expected Answer: Henry completed his M.S. in Machine Learning and Data Science at Northwestern University, graduating in December 2023.\n",
      "\n",
      "Question: When did he graduate from UCLA with a bachelor's degree?\n",
      "Generated Answer: To determine when Henry Liang graduated from UCLA, I need to look up his education record. Unfortunately, there is no specific record provided that lists his graduation date. However, based on the education details given, we can infer that his undergraduate studies were completed at UCLA, which would place him in the academic year ending in 2023.\n",
      "\n",
      "Since he started working in 2024, and assuming he has been employed since then, we can calculate the start date of his education. Given that he was a Data Scientist II starting in 2024, if we assume he had a Master’s degree before becoming a Data Scientist, this might be around 2023.\n",
      "\n",
      "Therefore, we can estimate that Henry Liang began his education at UCLA in 2023, with a Master's degree likely being part of his program.\n",
      "Expected Answer: He graduated from the University of California, Los Angeles, with a B.S. in Applied Mathematics and Statistics in March 2022.\n",
      "\n",
      "Question: What did Henry study for his undergraduate degree?\n",
      "Generated Answer: Henry studied for his undergraduate degree at Northwestern University. Specifically, he obtained his Master's degree in Machine Learning and Data Science in December 2023.\n",
      "Expected Answer: He studied Applied Mathematics and Statistics for his undergraduate degree.\n",
      "\n",
      "Question: What is his current role?\n",
      "Generated Answer: Henry Liang's current role is Data Scientist II at Vail Systems, Inc. He has been working on various projects such as creating a novel hybrid embedding reducing no-context rate for RFP completion, leading production of an FAQ chatbot, and extending the FAQ chatbot's functionality to support 10,000+ documents.\n",
      "Expected Answer: He is currently a Data Scientist II at Vail Systems, Inc., a role he began in February 2024.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What major achievement did Henry accomplish at Vail Systems?\n",
      "Generated Answer: Henry accomplished several significant achievements at Vail Systems:\n",
      "\n",
      "1. He was recognized for creating a novel hybrid embedding reducing no-context rate for RFP completion by 83.33%. This led to a paper acceptance at the AI-ML Systems Conference.\n",
      "\n",
      "2. He led the production of a FAQ chatbot using a RAG framework, achieving a top-5 retrieval accuracy of 99.18%.\n",
      "\n",
      "3. He reduced the FAQ system's latency by 65%, demonstrating improved performance.\n",
      "\n",
      "4. He optimized data curation, embedding optimization, and HNSW indexing techniques, resulting in a reduction in retrieval accuracy from 83.33% to 99.18%.\n",
      "\n",
      "5. He extended the FAQ chatbot's functionality to support up to 10,000 documents, including API documentation and code samples.\n",
      "\n",
      "These accomplishments highlight Henry's contributions to improving FAQ systems and enhancing overall user experience at Vail Systems.\n",
      "Expected Answer: Henry created a novel hybrid embedding that reduced the no-context rate for RFP completion by 83.33%, leading to a paper acceptance at the AI-ML Systems Conference in October 2024.\n",
      "\n",
      "Question: Where did he work before joining Vail Systems?\n",
      "Generated Answer: Before joining Vail Systems, Henry Liang worked as a Data Scientist II at Vail Systems, Inc.\n",
      "Expected Answer: Before joining Vail Systems, he worked as an Applied Scientist Intern at Amazon and a Data Science Intern at Roblox.\n",
      "\n",
      "Question: What tool did he develop during his Amazon internship?\n",
      "Generated Answer: During his Amazon internship, Henry Liang developed a custom LLM app to explain knowledge graph differences and detect network routing anomalies.\n",
      "Expected Answer: He developed a custom LLM app to explain knowledge graph differences and detect network routing anomalies, achieving 99% accuracy in summarizing path attributes with KGAG.\n",
      "\n",
      "Question: Which internship involved building an ETL pipeline for analyzing game metrics?\n",
      "Generated Answer: The internship involved building an ETL pipeline with Spark SQL for analyzing game-level metrics across 1,000,000,000+ observations.\n",
      "Expected Answer: Henry built an ETL pipeline for analyzing game-level metrics at Roblox during his Data Science Internship.\n",
      "\n",
      "Question: What was his role at Northwestern University?\n",
      "Generated Answer: Henry Liang held the role of a Data Scientist II at Northwestern University.\n",
      "Expected Answer: He was a Machine Learning Researcher at Northwestern University, where he designed and implemented a scalable AWS IoT pipeline for real-time forecasting.\n",
      "\n",
      "Question: What programming languages does Henry know?\n",
      "Generated Answer: Based on the provided information, Henry Liang knows several programming languages. His skills include Python, Java, and JavaScript, which are also mentioned as part of his programming background.\n",
      "Expected Answer: Henry is proficient in Python, R, PostgreSQL, Java, and JavaScript.\n",
      "\n",
      "Question: What machine learning tools is he skilled in?\n",
      "Generated Answer: Based on the provided information, Henry Liang is proficient in several machine learning tools:\n",
      "1. Python\n",
      "2. R\n",
      "3. PostgreSQL\n",
      "4. Java\n",
      "5. JavaScript\n",
      "6. TensorFlow\n",
      "7. PyTorch\n",
      "8. LangChain\n",
      "9. Hugging Face\n",
      "10. Git\n",
      "11. Docker\n",
      "12. Kubernetes\n",
      "13. Apache Spark\n",
      "14. Apache Hadoop\n",
      "Expected Answer: He is skilled in TensorFlow, PyTorch, LangChain, and Hugging Face.\n",
      "\n",
      "Question: What cloud and container tools does he use?\n",
      "Generated Answer: Based on the provided information, here are the cloud and container tools Henry Liang uses:\n",
      "- Python: He uses this programming language extensively for developing various applications.\n",
      "- Java: Although not specifically mentioned, Python may have been used for some parts of his work due to its widespread adoption and ease of integration.\n",
      "- PostgreSQL: This database management system is likely used for storing and querying large datasets.\n",
      "- JavaScript: He might also be using JavaScript for web development or other frontend technologies.\n",
      "- TensorFlow and PyTorch: These are machine learning libraries used for deep learning tasks.\n",
      "- Git: He might be using version control systems like Git for managing his project's codebase.\n",
      "- Docker: He might be using Docker containers for deploying his application.\n",
      "- Kubernetes: For managing containerized environments.\n",
      "- Apache Spark and Apache Hadoop: For handling large-scale data processing tasks.\n",
      "- LangChain: A natural language processing library.\n",
      "- Hugging Face: For building models and deploying them.\n",
      "- Hugging Face: For building models and deploying them.\n",
      "- Git: He might be using version control systems like Git for managing his project's codebase.\n",
      "- Docker: He might be using Docker containers for deploying his application.\n",
      "- Kubernetes: For managing containerized environments.\n",
      "- Apache Spark and Apache Hadoop: For handling large-scale data processing tasks.\n",
      "- LangChain: For building models and deploying them.\n",
      "- Hugging Face: For building models and deploying them.\n",
      "- Git: He might be using version control systems like Git for managing his project's codebase.\n",
      "- Docker: He might be using Docker containers for deploying his application.\n",
      "- Kubernetes: For managing containerized environments.\n",
      "- Apache Spark and Apache Hadoop: For handling large-scale data processing tasks.\n",
      "- LangChain: For building models and deploying them.\n",
      "- Hugging Face: For building models and deploying them.\n",
      "- Git: He might be using version control systems like Git for managing his project's codebase.\n",
      "- Docker: He might be using Docker containers for deploying his application.\n",
      "- Kubernetes: For managing containerized environments.\n",
      "- Apache Spark and Apache Hadoop: For handling large-scale data processing tasks.\n",
      "- LangChain: For building models and deploying them.\n",
      "- Hugging Face: For building models and deploying them.\n",
      "- Git: He might be using version control systems like Git for managing his project's codebase.\n",
      "- Docker: He might be using Docker containers for deploying his application.\n",
      "- Kubernetes: For managing containerized environments.\n",
      "- Apache Spark and Apache Hadoop: For handling large-scale data\n",
      "Expected Answer: He uses Docker, Kubernetes, and has experience with AWS IoT pipelines.\n",
      "\n",
      "Question: Which languages does Henry speak?\n",
      "Generated Answer: Based on the provided information, Henry speaks English and Mandarin.\n",
      "Expected Answer: Henry speaks English, Mandarin, and French.\n",
      "\n",
      "Question: What tools has he used for big data processing?\n",
      "Generated Answer: Henry Liang has used various tools for big data processing. Here's a summary based on the provided information:\n",
      "\n",
      "Programming languages:\n",
      "- Python\n",
      "- R\n",
      "- Java\n",
      "- JavaScript\n",
      "\n",
      "Tools:\n",
      "- TensorFlow\n",
      "- PyTorch\n",
      "- LangChain\n",
      "- Hugging Face\n",
      "- Git\n",
      "- Docker\n",
      "- Kubernetes\n",
      "- Apache Spark\n",
      "- Apache Hadoop\n",
      "\n",
      "These tools help him manage and process large datasets efficiently, which is crucial for conducting research and implementing machine learning models.\n",
      "Expected Answer: He has used Apache Spark and Apache Hadoop for big data processing.\n",
      "\n",
      "Average Hallucination Rate: 0.6530\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# def extract_entities(text):\n",
    "#     doc = nlp(text)\n",
    "#     return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "\n",
    "# Initialize metrics\n",
    "results = []\n",
    "\n",
    "for item in qa_dataset:\n",
    "    q = item[\"question\"]\n",
    "    expected_answer = item[\"answer\"]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": get_user_prompt(education_text, work_text, skills_text, q)}\n",
    "    ]\n",
    "    text = tokenizer_qwen.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer_qwen([text], return_tensors=\"pt\").to(model_qwen.device)\n",
    "\n",
    "    generated_ids = model_qwen.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer_qwen.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Log generated response\n",
    "    print(f\"Question: {q}\")\n",
    "    print(f\"Generated Answer: {response}\")\n",
    "    print(f\"Expected Answer: {expected_answer}\")\n",
    "    print()\n",
    "\n",
    "    prompt_entities, hypothesis_entities, hallucinated_entities, non_hallucinated_entities = get_all_entities(prompt=expected_answer, hypothesis=response) \n",
    "\n",
    "    if len(hypothesis_entities) == 0:\n",
    "        ratio = 0\n",
    "    else:\n",
    "        ratio = len(hallucinated_entities) / len(hypothesis_entities)\n",
    "    # Append to results\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"expected_answer\": expected_answer,\n",
    "        \"generated_answer\": response,\n",
    "        \"prompt entities\": prompt_entities,\n",
    "        \"hypothesis_entities\": hypothesis_entities,\n",
    "        \"hallucinated_entities\": hallucinated_entities,\n",
    "        \"non_hallucinated_entities\": non_hallucinated_entities,\n",
    "        \"hallucinated_entities_in_response\": ratio\n",
    "    })\n",
    "\n",
    "# Calculate average BLEU score\n",
    "average_hallucination_rate = sum([result[\"hallucinated_entities_in_response\"] for result in results]) / len(results)\n",
    "\n",
    "# Log evaluation metrics\n",
    "print(f\"Average Hallucination Rate: {average_hallucination_rate:.4f}\")\n",
    "\n",
    "# Optional: Save results to a file for further analysis\n",
    "import json\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
