{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/Desktop/qwen0.5b-dev/qwen_dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Qwen2.5-0.5B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Qwen2.5-0.5B-Instruct\", temperature = 0.1, top_p = 0.6, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the YAML data\n",
    "with open(\"./me.yaml\", \"r\") as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "# Process the education data\n",
    "education = data[\"education\"]\n",
    "education_text = \"\\n\".join(\n",
    "    [f\"- Degree: {e['degree']}, Institution: {e['institution']}, Graduation Date: {e['graduation_date']}\" for e in education]\n",
    ")\n",
    "\n",
    "# Process the work experience data\n",
    "work = data[\"work_experience\"]\n",
    "work_text = \"\\n\".join(\n",
    "    [\n",
    "        f\"- Role: {w['title']}, Company: {w['company']}, Start Date: {w['start_date']}, End Date: {w.get('end_date', 'Present')}, Work Description: {w['achievements']}\"\n",
    "        for w in work\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Process the skills data\n",
    "skills = data[\"skills\"]\n",
    "skills_text = (\n",
    "    f\"Programming: {skills['programming_languages']}, Tools: {skills['tools']}, Languages: {skills['languages']}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You are an AI assistant of Henry Liang.\n",
    "You are designed to assist users with questions about Henry Liang, his work, and his experience.\n",
    "\n",
    "**Tone and Scope:**\n",
    "Answer all questions cheerfully, but do not provide more information than what is explicitly asked.\n",
    "\n",
    "**Uncertainty:**\n",
    "If you do not know the answer, state clearly, \"I do not know.\" Avoid guessing or fabricating information.\n",
    "\n",
    "**Restrictions:**\n",
    "Do not answer questions that are inappropriate, harmful, racist, or illegal.\n",
    "Avoid using inappropriate language under any circumstance.\n",
    "Do not provide medical, legal, or financial advice.\n",
    "Do not share any information that can identify or locate a person.\n",
    "Follow these guidelines strictly, and always prioritize clarity, accuracy, and adherence to the scope of your role.\n",
    "'''\n",
    "\n",
    "def get_user_prompt(education_text, work_text, skills_text, q):\n",
    "    # Combine the information into the final content string\n",
    "    content = f\"\"\"\n",
    "    The following is data about Henry Liang:\n",
    "\n",
    "    **Education:**\n",
    "    {education_text}\n",
    "\n",
    "    **Work Experience:**\n",
    "    {work_text}\n",
    "\n",
    "    **Skills:**\n",
    "    {skills_text}\n",
    "\n",
    "    If it is a question about Henry Liang, use the above information to answer the following question about Henry Liang.\n",
    "\n",
    "    {q}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = [\n",
    "    # Education-related questions\n",
    "    {\n",
    "        \"question\": \"Where did Henry complete his master's degree?\",\n",
    "        \"answer\": \"Henry completed his M.S. in Machine Learning and Data Science at Northwestern University, graduating in December 2023.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When did he graduate from UCLA with a bachelor's degree?\",\n",
    "        \"answer\": \"He graduated from the University of California, Los Angeles, with a B.S. in Applied Mathematics and Statistics in March 2022.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What did Henry study for his undergraduate degree?\",\n",
    "        \"answer\": \"He studied Applied Mathematics and Statistics for his undergraduate degree.\"\n",
    "    },\n",
    "\n",
    "    # Work experience-related questions\n",
    "    {\n",
    "        \"question\": \"What is his current role?\",\n",
    "        \"answer\": \"He is currently a Data Scientist II at Vail Systems, Inc., a role he began in February 2024.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What major achievement did Henry accomplish at Vail Systems?\",\n",
    "        \"answer\": \"Henry created a novel hybrid embedding that reduced the no-context rate for RFP completion by 83.33%, leading to a paper acceptance at the AI-ML Systems Conference in October 2024.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Where did he work before joining Vail Systems?\",\n",
    "        \"answer\": \"Before joining Vail Systems, he worked as an Applied Scientist Intern at Amazon and a Data Science Intern at Roblox.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What tool did he develop during his Amazon internship?\",\n",
    "        \"answer\": \"He developed a custom LLM app to explain knowledge graph differences and detect network routing anomalies, achieving 99% accuracy in summarizing path attributes with KGAG.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which internship involved building an ETL pipeline for analyzing game metrics?\",\n",
    "        \"answer\": \"Henry built an ETL pipeline for analyzing game-level metrics at Roblox during his Data Science Internship.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was his role at Northwestern University?\",\n",
    "        \"answer\": \"He was a Machine Learning Researcher at Northwestern University, where he designed and implemented a scalable AWS IoT pipeline for real-time forecasting.\"\n",
    "    },\n",
    "\n",
    "    # Skills and tools-related questions\n",
    "    {\n",
    "        \"question\": \"What programming languages does Henry know?\",\n",
    "        \"answer\": \"Henry is proficient in Python, R, PostgreSQL, Java, and JavaScript.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What machine learning tools is he skilled in?\",\n",
    "        \"answer\": \"He is skilled in TensorFlow, PyTorch, LangChain, and Hugging Face.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What cloud and container tools does he use?\",\n",
    "        \"answer\": \"He uses Docker, Kubernetes, and has experience with AWS IoT pipelines.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which languages does Henry speak?\",\n",
    "        \"answer\": \"Henry speaks English, Mandarin, and French.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What tools has he used for big data processing?\",\n",
    "        \"answer\": \"He has used Apache Spark and Apache Hadoop for big data processing.\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-English\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-English\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Entities: [('December', 'DATE'), ('2023', 'DATE'), ('XYZ University', 'ORG')]\n",
      "Hypothesis Entities: [('2023', 'DATE'), ('ABC University', 'ORG'), ('PhD', 'WORK_OF_ART'), ('XYZ University', 'ORG')]\n",
      "Hallucinated Entities: [('ABC University', 'ORG'), ('PhD', 'WORK_OF_ART')]\n",
      "Non-Hallucinated Entities: [('2023', 'DATE'), ('XYZ University', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# prompt = '''He has used the following tools for big data processing:\n",
    "# - TensorFlow\n",
    "# - PyTorch\n",
    "# - LangChain\n",
    "# - Hugging Face\n",
    "# - Git\n",
    "# - Docker\n",
    "# - Kubernetes\n",
    "# - Apache Spark\n",
    "# - Apache Hadoop'''\n",
    "# hypothesis = \"You graduated in 2023 with a master's from ABC University, and currently doing a PhD at XYZ University.\"\n",
    "\n",
    "# # Prompt and hypothesis\n",
    "prompt = \"I graduated with a master's in December 2023 from XYZ University.\"\n",
    "hypothesis = \"You graduated in 2023 with a master's from ABC University, and currently doing a PhD at XYZ University.\"\n",
    "\n",
    "# Extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "def extract_dates(entities):\n",
    "    new_entities = []\n",
    "    for ent, label in entities:\n",
    "        if label == \"DATE\":\n",
    "            splits = ent.split(\" \")\n",
    "            for split in splits:\n",
    "                new_entities.append((split, label))\n",
    "        else:\n",
    "            new_entities.append((ent, label))\n",
    "    return new_entities\n",
    "\n",
    "prompt_entities = extract_dates(extract_entities(prompt))\n",
    "hypothesis_entities = extract_dates(extract_entities(hypothesis))\n",
    "\n",
    "# Compare entities\n",
    "hallucinated = [ent for ent in hypothesis_entities if ent not in prompt_entities]\n",
    "non_hallucinated = [ent for ent in hypothesis_entities if ent in prompt_entities]\n",
    "\n",
    "\n",
    "print(\"Prompt Entities:\", prompt_entities)\n",
    "print(\"Hypothesis Entities:\", hypothesis_entities)\n",
    "print(\"Hallucinated Entities:\", hallucinated)\n",
    "print(\"Non-Hallucinated Entities:\", non_hallucinated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where did Henry complete his master's degree?\n",
      "Generated Answer: Henry completed his Master's degree from Northwestern University in December 2023.\n",
      "Expected Answer: Henry completed his M.S. in Machine Learning and Data Science at Northwestern University, graduating in December 2023.\n",
      "\n",
      "Question: When did he graduate from UCLA with a bachelor's degree?\n",
      "Generated Answer: Henry Liang graduated from University of California, Los Angeles with a Bachelor of Arts degree in Applied Mathematics and Statistics on March 2022.\n",
      "Expected Answer: He graduated from the University of California, Los Angeles, with a B.S. in Applied Mathematics and Statistics in March 2022.\n",
      "\n",
      "Question: What did Henry study for his undergraduate degree?\n",
      "Generated Answer: Henry studied for his undergraduate degree at Northwestern University. Specifically, he graduated with a Master's degree in Machine Learning and Data Science in December 2023.\n",
      "Expected Answer: He studied Applied Mathematics and Statistics for his undergraduate degree.\n",
      "\n",
      "Question: What is his current role?\n",
      "Generated Answer: Based on the provided information, Henry Liang's current role is Data Scientist II at Vail Systems, Inc.\n",
      "Expected Answer: He is currently a Data Scientist II at Vail Systems, Inc., a role he began in February 2024.\n",
      "\n",
      "Question: What major achievement did Henry accomplish at Vail Systems?\n",
      "Generated Answer: Henry Liang's major achievement at Vail Systems was creating a novel hybrid embedding reducing no-context rate for RFP completion by 83.33%. This accomplishment led to paper acceptance at the AI-ML Systems Conference in October 2024.\n",
      "Expected Answer: Henry created a novel hybrid embedding that reduced the no-context rate for RFP completion by 83.33%, leading to a paper acceptance at the AI-ML Systems Conference in October 2024.\n",
      "\n",
      "Question: Where did he work before joining Vail Systems?\n",
      "Generated Answer: Before joining Vail Systems, Henry Liang worked as an applied scientist intern (Capstone) at Amazon from September 2023 to December 2023.\n",
      "Expected Answer: Before joining Vail Systems, he worked as an Applied Scientist Intern at Amazon and a Data Science Intern at Roblox.\n",
      "\n",
      "Question: What tool did he develop during his Amazon internship?\n",
      "Generated Answer: During his Amazon internships, Henry Liang developed several tools. Specifically:\n",
      "\n",
      "1. [Languagelab](https://github.com/username/languagelab) - This project allowed him to analyze and improve the quality of natural language processing (NLP) models used in various applications, such as sentiment analysis and text generation.\n",
      "\n",
      "2. [Hugging Face](https://huggingface.co/) - He utilized this platform to build and deploy machine learning models and solutions related to NLP tasks.\n",
      "\n",
      "3. [LangChain](https://langchain.apache.org/) - A library that enables developers to create conversational agents, language models, and other NLP applications.\n",
      "\n",
      "These projects demonstrate Henry's skills in developing and utilizing various tools and technologies relevant to his field.\n",
      "Expected Answer: He developed a custom LLM app to explain knowledge graph differences and detect network routing anomalies, achieving 99% accuracy in summarizing path attributes with KGAG.\n",
      "\n",
      "Question: Which internship involved building an ETL pipeline for analyzing game metrics?\n",
      "Generated Answer: The internship involving the creation of an ETL pipeline with Spark SQL was the Data Science Intern position at Roblox.\n",
      "Expected Answer: Henry built an ETL pipeline for analyzing game-level metrics at Roblox during his Data Science Internship.\n",
      "\n",
      "Question: What was his role at Northwestern University?\n",
      "Generated Answer: Henry Liang's role at Northwestern University was as a PhD student in the Department of Computer Science. He earned his master's degree in Machine Learning and Data Science from Northwestern University in December 2023.\n",
      "Expected Answer: He was a Machine Learning Researcher at Northwestern University, where he designed and implemented a scalable AWS IoT pipeline for real-time forecasting.\n",
      "\n",
      "Question: What programming languages does Henry know?\n",
      "Generated Answer: Henry knows several programming languages, including Python, R, PostgreSQL, Java, JavaScript, TensorFlow, PyTorch, LangChain, Hugging Face, Git, Docker, Kubernetes, Apache Spark, and Apache Hadoop.\n",
      "Expected Answer: Henry is proficient in Python, R, PostgreSQL, Java, and JavaScript.\n",
      "\n",
      "Question: What machine learning tools is he skilled in?\n",
      "Generated Answer: He is skilled in Python, R, PostgreSQL, Java, JavaScript, TensorFlow, PyTorch, LangChain, Hugging Face, Git, Docker, Kubernetes, Apache Spark, and Apache Hadoop.\n",
      "Expected Answer: He is skilled in TensorFlow, PyTorch, LangChain, and Hugging Face.\n",
      "\n",
      "Question: What cloud and container tools does he use?\n",
      "Generated Answer: Based on the provided information, Henry Liang primarily uses the following cloud and container tools:\n",
      "\n",
      "Cloud Tools:\n",
      "- Docker\n",
      "- Kubernetes\n",
      "\n",
      "Container Tools:\n",
      "- Python\n",
      "- R\n",
      "- PostgreSQL\n",
      "- Java\n",
      "- JavaScript\n",
      "- TensorFlow\n",
      "- PyTorch\n",
      "- LangChain\n",
      "- Hugging Face\n",
      "- Git\n",
      "- Docker\n",
      "- Kubernetes\n",
      "Expected Answer: He uses Docker, Kubernetes, and has experience with AWS IoT pipelines.\n",
      "\n",
      "Question: Which languages does Henry speak?\n",
      "Generated Answer: Henry speaks English, Mandarin, and French.\n",
      "Expected Answer: Henry speaks English, Mandarin, and French.\n",
      "\n",
      "Question: What tools has he used for big data processing?\n",
      "Generated Answer: He has used the following tools for big data processing:\n",
      "- TensorFlow\n",
      "- PyTorch\n",
      "- LangChain\n",
      "- Hugging Face\n",
      "- Git\n",
      "- Docker\n",
      "- Kubernetes\n",
      "- Apache Spark\n",
      "- Apache Hadoop\n",
      "Expected Answer: He has used Apache Spark and Apache Hadoop for big data processing.\n",
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m\n\u001b[1;32m     52\u001b[0m     non_hallucinated \u001b[38;5;241m=\u001b[39m [ent \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m hypothesis_entities \u001b[38;5;28;01mif\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m prompt_entities]   \n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Append to results\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: q,\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m: expected_answer,\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m: response,\n\u001b[0;32m---> 59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhallucinated_entities_in_response\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhallucinated\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhypothesis_entities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     })\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Calculate average BLEU score\u001b[39;00m\n\u001b[1;32m     63\u001b[0m average_hallucination_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhallucinated_entities_in_response\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(results)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize metrics\n",
    "results = []\n",
    "\n",
    "for item in qa_dataset:\n",
    "    q = item[\"question\"]\n",
    "    expected_answer = item[\"answer\"]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": get_user_prompt(education_text, work_text, skills_text, q)}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Log generated response\n",
    "    print(f\"Question: {q}\")\n",
    "    print(f\"Generated Answer: {response}\")\n",
    "    print(f\"Expected Answer: {expected_answer}\")\n",
    "    print()\n",
    "\n",
    "    prompt_entities = extract_entities(expected_answer)\n",
    "    hypothesis_entities = extract_entities(response)\n",
    "\n",
    "    # Simple similarity metric (BLEU score for QA)\n",
    "    hallucinated = [ent for ent in hypothesis_entities if ent not in prompt_entities]\n",
    "    non_hallucinated = [ent for ent in hypothesis_entities if ent in prompt_entities]   \n",
    "\n",
    "    # Append to results\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"expected_answer\": expected_answer,\n",
    "        \"generated_answer\": response,\n",
    "        \"hallucinated_entities_in_response\": len(hallucinated) / len(hypothesis_entities)\n",
    "    })\n",
    "\n",
    "# Calculate average BLEU score\n",
    "average_hallucination_rate = sum([result[\"hallucinated_entities_in_response\"] for result in results]) / len(results)\n",
    "\n",
    "# Log evaluation metrics\n",
    "print(f\"Average Hallucination Rate: {average_hallucination_rate:.4f}\")\n",
    "\n",
    "# Optional: Save results to a file for further analysis\n",
    "import json\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
